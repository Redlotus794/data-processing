{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27771ccf-ee42-4181-ba0f-8f5f9de15fd2",
   "metadata": {},
   "source": [
    "# DVL - Dragon Version Log V1\n",
    "\n",
    "这是一个使用全球整车发版日志进行搜索处理的，以期达到在少量数据上有较高的搜索准确率\n",
    "v1 使用的数据集是 未经过摘要的原始发版日志数据集 20240228版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6a1dde-b92c-4087-954e-9155d4c1151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9df298-fc6f-4545-9808-df4c5f09fd61",
   "metadata": {},
   "source": [
    "# 加载环境配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5099c105-d34f-4b6d-95a7-779cd8582c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env ENV_FOR_DYNACONF=local\n",
    "import os \n",
    "os.chdir('/Users/wangjialong/Documents/code/saic_project/global-vehicle-dragon/data-processing')\n",
    "\n",
    "# 重新加载模块\n",
    "from importlib import reload\n",
    "from config.settings import Settings\n",
    "import db.db_manager\n",
    "import config.settings\n",
    "import os\n",
    "reload(config.settings)\n",
    "reload(db.db_manager)\n",
    "print(f\"Debug: {bool(Settings.DEBUG)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfec5beb-0523-426b-89d3-24dae3fed211",
   "metadata": {},
   "source": [
    "# 查询源数据目录，加载到DB中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5bd88b-be46-4bda-9d96-7b56932be412",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_directory = r'/Users/wangjialong/Documents/AI/SAIC_DCSGlobalVehicle/DVL/prd_dvl_md_20240228'\n",
    "files = os.listdir(file_directory)\n",
    "file_count = sum(os.path.isfile(os.path.join(file_directory, item)) for item in files)\n",
    "print(f\"目录 '{file_directory}' 下有 {file_count} 个文件。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253260f5-a71d-497d-8918-1f8dff1a2558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from db.model.p_dragon import DVL\n",
    "import hashlib\n",
    "from etl.dragon_etl import save_dvl_model\n",
    "from db.db_manager import DBConn\n",
    "\n",
    "for index, item in enumerate(files, start=1):\n",
    "    if item in ['.DS_Store', 'index.md']:\n",
    "        continue;\n",
    "    \n",
    "    db_conn = DBConn()\n",
    "    full_path = os.path.join(file_directory, item)\n",
    "    hash_obj = hashlib.sha256()\n",
    "    if os.path.isfile(full_path):\n",
    "        print(f\"处理文件 {index} |'{item}'\")\n",
    "        try:\n",
    "            with open(full_path, 'r') as file:\n",
    "                content = file.read()\n",
    "                hash_obj.update(content.encode())\n",
    "                new_model = DVL(\n",
    "                    name = item,\n",
    "                    hash = hash_obj.hexdigest(),\n",
    "                    content = content\n",
    "                )\n",
    "            save_dvl_model(conn=db_conn, model=new_model)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed! {index} | {full_path} | {e}\")\n",
    "    db_conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08db7e9e-8f3d-42c2-ae0a-653fa37b9e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "def sentence_to_vector(sentence):\n",
    "    \"\"\"\n",
    "    定义模型转embedding\n",
    "    :return embedding\n",
    "    \"\"\"\n",
    "    # 加载分词器和模型\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "    model = BertModel.from_pretrained('bert-base-chinese')\n",
    "    input = tokenizer(sentence, return_tensors='pt', padding='max_length', max_length=128)\n",
    "    with torch.no_grad():\n",
    "        output = model(**input)\n",
    "    sentence_embedding = output.last_hidden_state.mean(dim=1)\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24da774-8d9d-4aef-86c5-4d04743526c5",
   "metadata": {},
   "source": [
    "# 对数据库内的数据进行向量化\n",
    "\n",
    "使用bert-base-chinese模型，对数据库的content进行分块嵌入到milvus的向量数据库中\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75efb8d-5cd7-49c3-b62c-aabe29cd9e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import (\n",
    "    MilvusClient,\n",
    "    connections,\n",
    "    utility,\n",
    "    FieldSchema, CollectionSchema, DataType,\n",
    "    Collection,\n",
    ")\n",
    "from sqlalchemy import func, create_engine\n",
    "from towhee import pipe, ops\n",
    "from db.milvus_manager import MilvusConn\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "import pandas as pd\n",
    "from db.db_manager import DBConn\n",
    "from db.model.p_dragon import DVL\n",
    "\n",
    "# 参数\n",
    "database_name = 'dvl_v1'\n",
    "# 最大文本的长度\n",
    "max_token = 512\n",
    "# 查询分区数量\n",
    "partition_num = 100\n",
    "# my_model_name = 'shibing624/text2vec-base-chinese'\n",
    "my_model_name = 'bert-base-chinese'\n",
    "\n",
    "connections.connect(host=Settings.MILVUS_URL, port=Settings.MILVUS_PORT)\n",
    "client = MilvusClient(host=Settings.MILVUS_URL, port=Settings.MILVUS_PORT)\n",
    "\n",
    "print(utility.has_collection(collection_name = database_name))\n",
    "if utility.has_collection(collection_name = database_name):\n",
    "    print(f\"describe: {client.describe_collection(collection_name = database_name)}\")\n",
    "    print(f\"num of entities: {client.num_entities(collection_name = database_name)}\")\n",
    "\n",
    "milvus_conn = MilvusConn(database_name)\n",
    "db_conn = DBConn()\n",
    "\n",
    "count_query = f\"SELECT count(1) FROM p_dragon.dragon_version_log LIMIT {partition_num}\"\n",
    "total_num = db_conn.session.query(func.count(DVL.id)).scalar()\n",
    "iteration_times = total_num // partition_num + (0 if total_num % partition_num == 0 else 1)\n",
    "print(f\"Records Count: {total_num} | iterations time: {iteration_times}\")\n",
    "db_conn.close()\n",
    "\n",
    "result_pd = pd.DataFrame(columns=['name', 'embedding'])\n",
    "\n",
    "# 这一块可以抽取相同代码块\n",
    "for i in range(iteration_times):    \n",
    "    engine = create_engine(Settings.DB_URL)\n",
    "    start_num = i * partition_num\n",
    "    select_query = f\"SELECT id, name, content as content FROM p_dragon.dragon_version_log LIMIT {partition_num} OFFSET {start_num}\"     \n",
    "    with engine.connect() as connection, connection.begin():\n",
    "        df = pd.read_sql_query(sql=select_query, con=connection.connection)\n",
    "        for index, row in enumerate(df.itertuples(index=False), start=1):\n",
    "            if row.id is None:\n",
    "                break\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if len(row.content) > max_token:\n",
    "                print(f\"{row.id} 大于最大token长度，切割块\")\n",
    "                markdown_spilitter = MarkdownTextSplitter(chunk_size=max_token, chunk_overlap=0)\n",
    "                docs = markdown_spilitter.create_documents([row.content])\n",
    "                for split in docs:\n",
    "                    normalize_vector = sentence_to_vector(split.page_content)\n",
    "                    new_row = pd.DataFrame([[row.name, normalize_vector[0].numpy()]],\n",
    "                        columns=['name', 'embedding'])\n",
    "                    result_pd = pd.concat([result_pd, new_row])\n",
    "            else:    \n",
    "                print(f\"处理数据 分片{i}|{index}\")\n",
    "                normalize_vector = sentence_to_vector(row.content)\n",
    "                new_row = pd.DataFrame([[row.name, normalize_vector[0].numpy()]],\n",
    "                        columns=['name', 'embedding'])\n",
    "                result_pd = pd.concat([result_pd, new_row])\n",
    "\n",
    "        milvus_conn.insert(result_pd)                \n",
    "        engine.dispose()\n",
    "milvus_conn.flush()\n",
    "milvus_conn.stats()\n",
    "print(f\"all done\") \n",
    "# print(f\"data 0 : {df.loc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "354ac879-393c-4b6b-9d90-c60ad69dde6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-28 15:17:49,565 - 140704323120000 - milvus_client.py-milvus_client:553 - DEBUG: Created new connection using: 0cffc1f8c5f14ff5880fe05daab79ccc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查询结果：['[\"id: 448029453523382943, distance: 50.983673095703125, entity: {\\'name\\': \\'1049852.md\\'}\", \"id: 448029453523383167, distance: 71.80381774902344, entity: {\\'name\\': \\'S8.7.20240103.PRD_13371786.md\\'}\", \"id: 448029453523383000, distance: 71.80381774902344, entity: {\\'name\\': \\'S9.3.20240202.PRD_13374147.md\\'}\", \"id: 448029453523383113, distance: 71.80381774902344, entity: {\\'name\\': \\'S7.10.20231110.PRD_9306660.md\\'}\", \"id: 448029453523383158, distance: 75.08514404296875, entity: {\\'name\\': \\'S6.13.20230915.PRD_1051659.md\\'}\"]']\n"
     ]
    }
   ],
   "source": [
    "# 根据简单的提示词，看下是否能正确的召回数据\n",
    "prompt_word = 'UK By Rule Allocation Scene的配车报告是哪个版本上线的？'\n",
    "\n",
    "embedding = sentence_to_vector(prompt_word)\n",
    "# print(embedding)\n",
    "\n",
    "# 参数\n",
    "database_name = 'dvl_v1'\n",
    "milvus_conn = MilvusConn(database_name)\n",
    "\n",
    "search_params = {\n",
    "    \"metric_type\": \"L2\", \n",
    "    \"offset\": 0, \n",
    "    \"ignore_growing\": False, \n",
    "    \"params\": {\"nprobe\": 10}\n",
    "}\n",
    "\n",
    "milvus_conn.collection.load()\n",
    "results = milvus_conn.collection.search(\n",
    "    data = [embedding[0].numpy()],\n",
    "    anns_field = 'embedding',\n",
    "    param = search_params,\n",
    "    limit = 5,\n",
    "    expr = None,\n",
    "    output_fields = ['name'],\n",
    "    consistency_level = 'Strong'\n",
    ")\n",
    "\n",
    "print(f\"查询结果：{results}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
